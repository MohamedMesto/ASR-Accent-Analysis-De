{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Change this if you don't want the data to be extracted in the current directory.\n",
    "data_dir = '.'\n",
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from scipy.io import wavfile\n",
    "from itertools import chain \n",
    "import string\n",
    "import Levenshtein as Lev\n",
    "from itertools import groupby\n",
    "import scipy.stats as st\n",
    "from scipy import signal\n",
    "import nltk\n",
    "from scipy.stats import wasserstein_distance as wd\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from pyemd import emd\n",
    "from hyphenate import hyphenate_word\n",
    "from itertools import islice \n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mmm2050/QU_DFKI_Thesis/Experimentation/ASR_Accent_Analysis_De'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################################\n",
    "########### set the Data files paths on Conda Notebook on Ubuntu #################### \n",
    "\n",
    "### \n",
    "os.chdir('/home/mmm2050/QU_DFKI_Thesis/Experimentation/ASR_Accent_Analysis_De')\n",
    "main_path=os.getcwd()\n",
    "\n",
    "if not os.path.exists(main_path+'/Data_results'):\n",
    "  os.makedirs(main_path+'/Data_results')\n",
    "\n",
    "if not os.path.exists(main_path+'/Figures_results'):\n",
    "  os.makedirs(main_path+'/Figures_results')\n",
    "\n",
    "Data_path=main_path+'/Data/'\n",
    "Data_results_path=main_path+'/Data_results/'\n",
    "json_file_path=Data_path+'results.json'\n",
    "json_file_path_small=Data_path+'results_small.json'\n",
    "validated_tsv_path=Data_path+'validated.tsv'\n",
    "validated_tsv_path_small=Data_path+'validated_small.tsv'\n",
    " \n",
    "\n",
    "validated_en_tsv_path=main_path+'/DeepSpeech/data/validated.tsv'\n",
    "validated_en_tsv_path_small=main_path+'/DeepSpeech/data/validated_small.tsv'\n",
    "\n",
    " \n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"00ff00\">  **To count the `number of occurrences` of each `word` over the sentences located in the transcripts column of a data frame:**</font>\n",
    "<font color=white> **the following steps are achieve that**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mmm2050/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the number of occurrences of each word over the sentences located in the transcripts column of a data frame:\n",
      "               count\n",
      "die            14029\n",
      "der            13302\n",
      "ist             7629\n",
      "und             7554\n",
      "in              7195\n",
      "...              ...\n",
      "kunstdünger        1\n",
      "hovan              1\n",
      "rocky              1\n",
      "inbesitznahme      1\n",
      "kippe              1\n",
      "\n",
      "[54083 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# First, import the required libraries. You will need the pandas library to work with data frames and the nltk library to tokenize the sentences and words in the transcripts column.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#@title ###**To count the number of occurrences of each word over the sentences located in the transcripts column of a data frame:**\n",
    "\n",
    "###########################################################################################################################################################\n",
    "########### creat a Dataframe called \"dataset_audiofilename_transcript_accent\" contains all audiofilename and thier 'transcript' and 'test_file' /Accent####\n",
    "###########################################################################################################################################################\n",
    "list_dataset_test_=['test_at.txt','test_gb.txt','test_it.txt','test_de_al.txt','test_fr.txt','test_de_ni.txt','test_ch.txt',\n",
    "               'test_de.txt','test_us.txt','test_ca.txt','test_ru.txt']\n",
    "list_dataset_test_accent=['Österreichisches Deutsch','Britisches Deutsch','Italienisch Deutsch','Alemannische Färbung,Schweizer Standart Deutsch',\n",
    "'Französisch Deutsch','Niederländisch Deutsch','Schweizerdeutsch','Deutschland Deutsch','Amerikanisches Deutsch','Kanadisches Deutsch','Russisch Deutsch']\n",
    "\n",
    "data_dict_accent_duration  = dict(zip( list_dataset_test_, list_dataset_test_accent))  \n",
    "\n",
    "# create an empty set to store the words that meet all 11 accents\n",
    "\n",
    "\n",
    "accent_long_list=[]\n",
    " \n",
    "\n",
    "### here is the required_word\n",
    "# required_word=\"Start\"\n",
    "# required_word=input('Insert a word to test it please? ')\n",
    "newfile=0\n",
    "\n",
    "\n",
    "# dict_audiofilename_transcript_accent=[]\n",
    "# To find out the Audio file's Accent\n",
    "list_test_accent_txt_values=[]\n",
    "list_test_accent_txt_keys=[]\n",
    "list_test_file=[]\n",
    "list_accent_long=[]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Dict_results = json.load(open(json_file_path))\n",
    "  with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    Dict_results = json.load(f)\n",
    "\n",
    "  for test_file in Dict_results:\n",
    "    # print(test_file)\n",
    "    list_test_accent_txt_values_temp = [v[\"reference\"] for v in Dict_results[test_file].values()]\n",
    "    list_test_accent_txt_keys_temp=[re.split(r'[.|/]',k)[9] for k in Dict_results[test_file].keys()]\n",
    "    list_test_file_temp=[test_file for v in Dict_results[test_file].values()]\n",
    "\n",
    "# creat a Dataframe called  dataset_audiofilename_transcript_accent contains all audiofilename and thier 'transcript' and 'test_file' /Accent\n",
    "    list_test_accent_txt_values.extend(list_test_accent_txt_values_temp)\n",
    "    list_test_accent_txt_keys.extend(list_test_accent_txt_keys_temp)\n",
    "    list_test_file.extend(list_test_file_temp)\n",
    "    keys=['audiofilename','transcript','test_file']\n",
    "    trans_dict_test_file_result = dict(zip(keys,[list_test_accent_txt_keys, list_test_accent_txt_values,list_test_file]))\n",
    "    \n",
    "dataset_audiofilename_transcript_accent = pd.DataFrame(trans_dict_test_file_result)\n",
    " \n",
    "# Tokenize the sentences in the transcripts column using the sent_tokenize function from the nltk library. This will create a list of sentences.\n",
    "sentences = dataset_audiofilename_transcript_accent['transcript'].apply(sent_tokenize)\n",
    "\n",
    "# Tokenize the words in each sentence using the word_tokenize function from the nltk library. This will create a list of words for each sentence.\n",
    "words = sentences.apply(lambda x: [word_tokenize(sentence) for sentence in x])\n",
    "\n",
    "# Flatten the list of words so that you have a single list of all words in the transcripts column.\n",
    "all_words = [word for sentence in words for word_list in sentence for word in word_list]\n",
    "\n",
    "# Use the Counter function from the collections library to count the occurrences of each word.\n",
    "from collections import Counter\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Convert the word_counts object to a data frame using the pd.DataFrame function.\n",
    "word_counts_dataset_audiofilename_transcript_accent = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count'])\n",
    "\n",
    "# Sort the data frame by the count column in descending order.\n",
    "word_counts_dataset_audiofilename_transcript_accent = word_counts_dataset_audiofilename_transcript_accent.sort_values('count', ascending=False)\n",
    "\n",
    "print('Count the number of occurrences of each word over the sentences located in the transcripts column of a data frame:')\n",
    "print(word_counts_dataset_audiofilename_transcript_accent)\n",
    "word_counts_dataset_audiofilename_transcript_accent.to_csv(Data_results_path+'number_of_occurrences_of_each_word.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
