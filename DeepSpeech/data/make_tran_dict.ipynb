{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from scipy.io import wavfile\n",
    "from itertools import chain \n",
    "import string\n",
    "import Levenshtein as Lev\n",
    "from itertools import groupby\n",
    "import scipy.stats as st\n",
    "from scipy import signal\n",
    "import nltk\n",
    "from scipy.stats import wasserstein_distance as wd\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from pyemd import emd\n",
    "from hyphenate import hyphenate_word\n",
    "from itertools import islice \n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "########### set the Data files paths on Conda Notebook on Ubuntu #################### \n",
    "\n",
    "### \n",
    "os.chdir('/home/mmm2050/QU_DFKI_Thesis/Experimentation/ASR_Accent_Analysis_De')\n",
    "main_path=os.getcwd()\n",
    "\n",
    "if not os.path.exists(main_path+'/Data_results'):\n",
    "  os.makedirs(main_path+'/Data_results')\n",
    "\n",
    "if not os.path.exists(main_path+'/Figures_results'):\n",
    "  os.makedirs(main_path+'/Figures_results')\n",
    "\n",
    "Data_path=main_path+'/Data/'\n",
    "Data_results_path=main_path+'/Data_results/'\n",
    "json_file_path=Data_path+'results.json'\n",
    "json_file_path_small=Data_path+'results_small.json'\n",
    "validated_tsv_path=Data_path+'validated.tsv'\n",
    "validated_tsv_path_small=Data_path+'validated_small.tsv'\n",
    " \n",
    "\n",
    "validated_en_tsv_path=main_path+'/DeepSpeech/data/validated.tsv'\n",
    "validated_en_tsv_path_small=main_path+'/DeepSpeech/data/validated_small.tsv'\n",
    "\n",
    " \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title read in all three CSV files at once and store them in separate dataframes\n",
    "\n",
    "\n",
    "# # my_data/all_overlap.txt ==>> test_at.txt\n",
    "# # transcript ==>> text\n",
    "# # file==>> audio_filepath\n",
    "# # all_overlap ==>> dataset_trans_test_all_duration_test_all_output.csv\n",
    "# ###############################################\n",
    "\n",
    "# df_at, df_gb, df_it, df_de_al, df_fr, df_de_ni, df_ch, df_de, df_us, df_ca, df_ru = pd.read_csv(['test_at.txt', 'test_gb.txt', 'test_it.txt', 'test_de_al.txt', 'test_fr.txt',\n",
    "#                                'test_de_ni.txt', 'test_ch.txt', 'test_de.txt', 'test_us.txt', 'test_ca.txt', 'test_ru.txt'] )\n",
    "\n",
    "MCV_all=pd.DataFrame({})\n",
    "\n",
    "\n",
    "df_at= pd.read_csv(Data_path+'test_at.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_gb= pd.read_csv(Data_path+'test_gb.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_it= pd.read_csv( Data_path+'test_it.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_de_al= pd.read_csv(Data_path+'test_de_al.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_fr= pd.read_csv(Data_path+'test_fr.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_de_ni= pd.read_csv( Data_path+'test_de_ni.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_ch= pd.read_csv(Data_path+'test_ch.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_de= pd.read_csv(Data_path+'test_de.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_us= pd.read_csv( Data_path+'test_us.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_ca= pd.read_csv(Data_path+'test_ca.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    "df_ru= pd.read_csv(Data_path+'test_ru.txt',delimiter = \", \", header = None ,encoding=\"utf-8\", names=['audio_filepath','transcript','duration'],engine='python' )\n",
    " \n",
    "# combine the dataframes into a single one \n",
    "combined_df = pd.concat([df_at, df_gb, df_it, df_de_al, df_fr, df_de_ni, df_ch, df_de, df_us, df_ca, df_ru], ignore_index=True)\n",
    "\n",
    "dataset_trans_test_all_duration=combined_df\n",
    "\n",
    "dataset_trans_test_all_duration['audio_filepath'] = dataset_trans_test_all_duration['audio_filepath'].map(lambda x: x.split('.',6)[2])\n",
    "dataset_trans_test_all_duration['audio_filepath'] = dataset_trans_test_all_duration['audio_filepath'].map(lambda x: x.split('/',6)[3])\n",
    "dataset_trans_test_all_duration['transcript'] = dataset_trans_test_all_duration['transcript'].map(lambda x: x.split('\": \"',2)[1])\n",
    "dataset_trans_test_all_duration['transcript'] = dataset_trans_test_all_duration['transcript'].map(lambda x: x.split('\"',2)[0])\n",
    "dataset_trans_test_all_duration['duration'] = dataset_trans_test_all_duration['duration'].map(lambda x: x.split('\": ',2)[1])\n",
    "dataset_trans_test_all_duration['duration'] = dataset_trans_test_all_duration['duration'].map(lambda x: x.split('}',2)[0])\n",
    "    \n",
    "\n",
    "print(type(dataset_trans_test_all_duration),'\\n',dataset_trans_test_all_duration.head(10))\n",
    "\n",
    "print(len(dataset_trans_test_all_duration))\n",
    "# the complete final dataframe of all Accents txt files\n",
    "dataset_trans_test_all_duration.to_csv( Data_results_path+'dataset_trans_test_all_duration.csv')\n",
    "\n",
    "MCV_all['audio_filepath']=dataset_trans_test_all_duration['audio_filepath']\n",
    "MCV_all['transcript']=dataset_trans_test_all_duration['transcript']\n",
    "\n",
    "MCV_all.to_csv( Data_results_path+'MCV_all.csv')\n",
    "\n",
    "\n",
    "dataset_trans_test_all_duration['transcript']\n",
    "\n",
    "transcripts = list(set(dataset_trans_test_all_duration['transcript'].tolist()))\n",
    "len(transcripts)\n",
    "trans_dict = {x:[] for x in transcripts}\n",
    "\n",
    "# Assign the row['transcript'] value to the \n",
    "for index, row in dataset_trans_test_all_duration.iterrows():\n",
    "    trans_dict[row['transcript']].append(row['audio_filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_dict['schreib ihr halt ein paar liebe worte rein']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
