{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Change this if you don't want the data to be extracted in the current directory.\n",
    "data_dir = '.'\n",
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from scipy.io import wavfile\n",
    "from itertools import chain \n",
    "import string\n",
    "import Levenshtein as Lev\n",
    "from itertools import groupby\n",
    "import scipy.stats as st\n",
    "from scipy import signal\n",
    "import nltk\n",
    "from scipy.stats import wasserstein_distance as wd\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from pyemd import emd\n",
    "from hyphenate import hyphenate_word\n",
    "from itertools import islice \n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mmm2050/QU_DFKI_Thesis/Experimentation/ASR_Accent_Analysis_De'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################################\n",
    "########### set the Data files paths on Conda Notebook on Ubuntu #################### \n",
    "\n",
    "### \n",
    "os.chdir('/home/mmm2050/QU_DFKI_Thesis/Experimentation/ASR_Accent_Analysis_De')\n",
    "main_path=os.getcwd()\n",
    "\n",
    "if not os.path.exists(main_path+'/Data_results'):\n",
    "  os.makedirs(main_path+'/Data_results')\n",
    "\n",
    "if not os.path.exists(main_path+'/Figures_results'):\n",
    "  os.makedirs(main_path+'/Figures_results')\n",
    "\n",
    "Data_path=main_path+'/Data/'\n",
    "Data_results_path=main_path+'/Data_results/'\n",
    "json_file_path=Data_path+'results.json'\n",
    "json_file_path_small=Data_path+'results_small.json'\n",
    "validated_tsv_path=Data_path+'validated.tsv'\n",
    "validated_tsv_path_small=Data_path+'validated_small.tsv'\n",
    " \n",
    "\n",
    "validated_en_tsv_path=main_path+'/DeepSpeech/data/validated.tsv'\n",
    "validated_en_tsv_path_small=main_path+'/DeepSpeech/data/validated_small.tsv'\n",
    "\n",
    " \n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Show `Existence` of `Word` in `11 German Accents`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfortunately, the Word \"Universum\" are ***NOT*** found in all German accents\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#@title ###**According to a given word, this code shows and writes in which Audio files and which Accent this given word is located:**\n",
    "\n",
    "###########################################################################################################################################################\n",
    "########### creat a Dataframe called \"dataset_audiofilename_transcript_accent\" contains all audiofilename and thier 'transcript' and 'test_file' /Accent####\n",
    "###########################################################################################################################################################\n",
    "list_dataset_test_=['test_at.txt','test_gb.txt','test_it.txt','test_de_al.txt','test_fr.txt','test_de_ni.txt','test_ch.txt',\n",
    "               'test_de.txt','test_us.txt','test_ca.txt','test_ru.txt']\n",
    "list_dataset_test_accent=['Österreichisches Deutsch','Britisches Deutsch','Italienisch Deutsch','Alemannische Färbung,Schweizer Standart Deutsch',\n",
    "'Französisch Deutsch','Niederländisch Deutsch','Schweizerdeutsch','Deutschland Deutsch','Amerikanisches Deutsch','Kanadisches Deutsch','Russisch Deutsch']\n",
    "\n",
    "data_dict_accent_duration  = dict(zip( list_dataset_test_, list_dataset_test_accent))  \n",
    "# create an empty set to store the words that meet all 11 accents\n",
    "words = set()\n",
    "accent_long_set=set()\n",
    "accent_long_list=[]\n",
    "##################################################\n",
    "# required_word=input('Insert a word to test it please? ')   # active this code line , to the given word changeable!!!\n",
    "required_word='Universum'\n",
    "newfile=0\n",
    " \n",
    "list_test_accent_txt_values=[]\n",
    "list_test_accent_txt_keys=[]\n",
    "list_test_file=[]\n",
    "list_accent_long=[]\n",
    "if __name__ == \"__main__\":\n",
    "  # Dict_results = json.load(open(Data_path+'results.json'))\n",
    "  with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    Dict_results = json.load(f)\n",
    "  for test_file in Dict_results:\n",
    "    # print(test_file)\n",
    "    list_test_accent_txt_values_temp = [v[\"reference\"] for v in Dict_results[test_file].values()]\n",
    "    list_test_accent_txt_keys_temp=[re.split(r'[.|/]',k)[9] for k in Dict_results[test_file].keys()]\n",
    "    list_test_file_temp=[test_file for v in Dict_results[test_file].values()]\n",
    "\n",
    "# creat a Dataframe called  dataset_audiofilename_transcript_accent contains all audiofilename and thier 'transcript' and 'test_file' /Accent\n",
    "    list_test_accent_txt_values.extend(list_test_accent_txt_values_temp)\n",
    "    list_test_accent_txt_keys.extend(list_test_accent_txt_keys_temp)\n",
    "    list_test_file.extend(list_test_file_temp)\n",
    "    keys=['audiofilename','transcript','test_file']\n",
    "    trans_dict_test_file_result = dict(zip(keys,[list_test_accent_txt_keys, list_test_accent_txt_values,list_test_file]))\n",
    "dataset_audiofilename_transcript_accent = pd.DataFrame(trans_dict_test_file_result)\n",
    "\n",
    "for key, row in dataset_audiofilename_transcript_accent.iterrows():\n",
    "  if required_word in row['transcript']:\n",
    "# to show the full name accent of the founded result \n",
    "    for keys_accent_long_i, values_accent_long_i in data_dict_accent_duration.items():\n",
    "      if dataset_audiofilename_transcript_accent.test_file[key]==keys_accent_long_i:\n",
    "        accent_long_list.append(data_dict_accent_duration[keys_accent_long_i])\n",
    "        accent_long_str = ', '.join(accent_long_list)\n",
    "        accent_long_set=set(accent_long_str.split(', '))\n",
    "        # print(accent_long_str)\n",
    "if len(accent_long_set) == 11:\n",
    "  print('*'*60)\n",
    "  print(f'Perfect, the Word {required_word} are found in all German accents')\n",
    "  print('*'*60)\n",
    "else:\n",
    "  print(f'unfortunately, the Word \"{required_word}\" are ***NOT*** found in all German accents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
